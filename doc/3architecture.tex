\newcommand{\archid}{1}
\chapter{Design of IoT monitoring platform architecture}
\label{ch:architecture}
%What needs to be monitored: sensor-networks/IoT 
%How data and information streams flow and combine 
%will focus on data sources and streams as to ascertain which architecture, configuration and components are needed/suitable.
This chapter will detail the process taken in order to device the platform and its architecture. This will be accomplished by first exploring the general problem domain. Subsequently the design of the proposed platform and its implementation will be deliberated by identifying the available supporting technologies, clarifying the adaptations made to those technologies and explaining further implementation details. The chapter will be concluded by discussing the advantages, limitations and considerations of the proposed solution.
\section{Goal}
Large sensor applications send immense amounts of low-level raw monitoring data that requires capturing and enriching. Individual snapshots of raw data might contain very little information. However when accumulated, these snapshots contain the potential from which meaningful conclusions can be derived. These decision range from single sensor scale to the sensor application as a whole. This raw data is enriched by combining and analysing datasets of similar, related data, in order to achieve a higher degree of information. The goal of the efforts described in this chapter is to conceive a software platform that enables software developers to construct their own sensor application monitoring system. The intention to achieve this is by devising a generic application backbone and base building blocks for  developers to extend and compose.  
\section{Conceptualization of the problem domain}
In this section the problem domain will be investigated in order to eventually determine the requirements for the model. This will achieved by performing a commonality/variability analysis (C/V analysis) of the problem domain, as described in section \ref{sec:back:cv_analysis}. The analysis consists of three concepts:
\begin{itemize}
\nospace
\item The definitions that will be used in the analysis and the remainder of this chapter, 
\item the common features shared by all elements in the problem domain and which may be assumed as established concepts, and 
\item the variations that appear between aspects of the problem domain for which must be accounted for in the proposed solution.
\end{itemize}
\subsubsection*{Definitions}
Firstly, some key terms will be defined that will be used in the analysis and the remainder of this chapter.
\begin{description}[style=nextline]
\nospace
\item[Platform] The monitoring platform to be designed.
\item[Application] The application that is being investigated by the platform.
\item[Snapshot] A message containing a collection of data-points indicating the state of a system on a certain instant.
\item[Source] An entity emitting a snapshot. This can be a physical end-device, external service or an process internal to the platform.
\item[Consequence] An action effected by the platform based on the analysis of one or more snapshots.
\end{description}
\subsubsection*{Commonalities}
With the definitions established some common features shared by each application in the problem domain will be identified next. These commonalities may be presumed during the design of the platform and grants a scope to the design efforts.
\begin{enumerate}[label=C\archid .\arabic*]
\nospace
\item \label{c:scale_sensor} The group of target applications involves a huge amount of sensors ([scale] which entails a high throughput of snapshots requiring analysis by the platform.
\item \label{c:snapshot} As mentioned in the definitions, data is captured in snapshots. These represent the (partial) state of the application as measured or determined at a certain point in time. These snapshots can be used for both input of the platform as for representing intermediary computation states.
\item \label{c:snapshot_transformation} The parameters and values of a snapshot, and therefore consecutive derived values, may be considered fixed. Parameters can only change by outputting a new snapshot, not during evaluation of the current one.
\end{enumerate}
\subsubsection*{Variabilities}
Finally, the variety within the problem domain will be explored. As the purpose of the solution is to process information the analysis will mostly focus on the variations in the domain of data and information produced by applications. The solution should provide proficient adaptability in order to account for this variability. This will be ensured by captivating these variations in requirements.
\begin{enumerate}[label=V\archid .\arabic*]
\nospace
\item \label{v:qoi} The first variety  encountered is the variation in Quality of Information (QoI). As described in the Background chapter (section \ref{sec:back:qoi}) there are many parameters characterizing the QoI of data. QoI can vary on any combination of them.
\item \label{v:conclusion_basis} Secondly, there is the information base on which conclusions are made. The first conclusion basis is elementary:
\begin{enumerate}
\nospace
\item Single snapshot. (e.g. a sensor requiring maintenance)
\end{enumerate}
The second identified analysis is based on a large amount of low-information snapshots \cite{qos_difficult}, of which two types are identified:
\begin{enumerate}[resume]
\item Multiple sequentially relevant snapshots from a single source (longitudinal), used to analyse tendency of parameters. (e.g. a sharp continuous increase in bandwidth used which may imply future capacity issues.)
\item Many multi-source snapshots without individual significance (lateral). E.g: while the individual bandwidth usage of sensors may be of little interest, knowledge of the average and total bandwidth usage of the system may be warranted.
\end{enumerate}
\item \label{v:consequence} The possible consequences by the platform have a large range of implementations and cannot be fully anticipated. However, though the exact implementation of consequences can never be anticipated exactly, some groups of consequences can be identified.
\begin{enumerate}
\nospace
\item Build a model for reporting purposes. In order to generate reports some high-level information data-points need to be calculated based on large datasets. these data-points are then exposed either by an in-memory component with an API or by persisting it to intermediary permanent storage.
\item Analyses which invoke immediate responses to the application or a command \& control service administrating the application.
\item Alerting or reporting according to a specified rule. When this user defined rule is met or violated an alert is sent to a maintenance operator or auxiliary system.
\end{enumerate}
\end{enumerate}
The final variety is the scale of the application. It has already been established that the platform will operate on applications of very large scale, i.e. thousands of sensors. However, given a thousand as lower bound, the upper bound is still uncertain. Therefore the size of the application is still uncertain and differing degrees of size require different computational needs.
\begin{enumerate}[label=V\archid .\arabic* , resume]
\nospace
\item \label{v:scale} The scale of large wireless sensor applications varies wildly. This yields for both the number of devices in the application and the rate at which the devices emit data.
\end{enumerate}
\section{Requirements for the proposed software platform}
In this section the requirements of the proposed platform will be described, in accordance with the variability identified in the previous section. 
\begin{enumerate}[label=R\archid .\arabic*]
\nospace
\item \label{r:snaptshot_transformation} The platform should enable the capture and transformation of snapshots.
\item \label{r:basis_single} The platform should enable processing of single snapshot.
\item \label{r:basis_historic} The platform should enable processing of a window of homogeneous snapshots.
\item \label{r:basis_accumulated} The platform should enable processing and aggregation of an enormous amount of snapshots.
\item \label{r:consequence} The platform should enable implementation of a wide range of consequences. It should at least provide for these anticipated types of consequence:
\begin{itemize}
\nospace
\item report building
\item application feedback
\item alerts of behavioural violations
\end{itemize}
\item \label{r:scale} the platform should be scalable in order to support any large amount of inputs
\end{enumerate}

\subsubsection*{Justification}
This section will be concluded by justifying the identified requirements according to the earlier performed C/V analysis. A formal traceability between the requirements, commonalities and variability is listed in table \ref{table:3_justification}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|} \hline
Requirement & Variability &  Commonality \\ \hline
\ref{r:snaptshot_transformation} & \ref{v:qoi} & \ref{c:snapshot}, \ref{c:snapshot_transformation}\\ \hline
\ref{r:basis_single} & \ref{v:conclusion_basis}a & \\ \hline
\ref{r:basis_historic} & \ref{v:conclusion_basis}b & \\ \hline
\ref{r:basis_accumulated} & \ref{v:conclusion_basis}c & \\ \hline
\ref{r:consequence} & \ref{v:consequence} & \\ \hline
\ref{r:scale} & \ref{v:scale} & \ref{c:scale_sensor} \\ \hline
\end{tabular}
\caption{traceability table for justification of requirements}
\label{table:3_justification}
\end{table}

The first requirement (\ref{r:snaptshot_transformation}) regards the definition and concepts of snapshots and is based on the commonalities and the variation in Quality of Information (Section \ref{sec:back:qoi}). As illustrated by the traceability table, the following three requirements (\ref{r:basis_single}--\ref{r:basis_accumulated}) closely correlate with the three varieties identified in \ref{v:conclusion_basis}. Requirement \ref{r:consequence} attempts to captivate the variability described in \ref{v:consequence}. This variation is captured in a single requirement as opposed to differentiating them as for \ref{v:conclusion_basis}. This is because the possible consequences are not limited to the identified consequence groups. Therefore, they are grouped into one abstract requirement. Lastly, the final requirement considers the scale of the target applications. This regards both the amount of devices in the target application as the frequency the send their snapshots. 

\section{Exploration of the solution domain}
This section will explore the solutions and supporting technologies that are offered. First, the  base architecture type will be considered of the platform, as it is the most fundamental decision to be made. Continuing, options for supporting technology will be explored. The section will be concluded by examining some distributed computing technologies. These technologies should enable data-intensive computations by distributing them over a cluster, as to provide the required scalability.
\subsection{Architecture and execution platform}
\subsubsection*{High level architecture}
The first decision to make is the high-level architecture to adopt. The first option for which is to implement the platform as a monolithic software system. The benefit of such a system is that it keeps the solution as simple as can be. This is reflected by a famous proverb of Edsger Dijkstra: ``Simplicity is a prerequisite for reliability'' \cite{dijkstra}. This simplicity entails a better understanding of the product by any future contributor or user, without the need to consult complex, detailed documentation. However monolithic software products have been known to be difficult to maintain, because code evolution becomes more difficult as development progresses and changes and additions are made to the code base. Additionally, monolithic software systems are notoriously difficult to scale and balance \cite{mono_vs_micro}, which violates requirement \ref{r:scale}. Therefore, instead distributed micro-component approach will be adopted.

Converse to the monolith is the micro-component architecture. It consists of a multitude of smaller components that are functionally distinct. These components communicate to one another through a underlying message distribution system. By functionally encapsulating the application into distinct modules, an inherent separation of concerns is achieved. This in turn reduces entangulation and improves the application's capacity for evolution. Micro-components are  more flexible than monoliths, allow for better functional composition, are easier to maintain and much more scalable \cite{mono_vs_micro}. Additionally, distributed cloud computing solves some of the tenacious obstacles in IoT's, such as the constraint computational and storage capacity \cite{benefits_cloud_to_iot}. The remainder of this section will investigate some established technologies enabling micro-service architectures.

\subsubsection*{Apache Storm}
Apache Storm is a big-data streaming library especially designed for separation of concerns and scalability. It achieves distributed compution by partitioning the stages of computation. It separates stages of computation in distinct processors performing a portion of the global process. These processors are composed into a topology. This topology specifies which processors communicate to which other processors using Storm's inherent message broker. By breaking up the computation, different stages can be distributed among machines and duplicated if required. The Storm topology consists of three chief concepts.
\begin{description}[style=nextline]
\nospace
\item[Spouts] Nodes that introduce data in the topology,
\item[Bolts] Nodes that perform some computation or transformation on data, and
\item[Topology] An application-level specification of how nodes are connected and messages distributed.
\end{description}
The computation is regarded as a directed graph with spouts and bolts as vertices, and an internalized messaging system as edges.

%By breaking up the computations into multiple consecutive bolts, Storm allows computations to be spread over a cluster. Additionally Storm allows individual bolts to be replicated and executed separately. This lateral distribution reduces the occurrence of bottlenecks in the network due to bolts executing expensive pivotal processes

Storm is especially suited for the purpose of this study since it was designed for interconnected micro-components. By employing Apache Storm both the distributed computation environment as the means of data distribution are obtained, simplifying the technology stack.

However, the built-in stream distribution mechanism is completely internalized, complicating integration with auxiliary processes. Tasks such as data injection, platform monitoring and data extraction for debugging, processing or reporting by third-party programs and stakeholders will require an exposing mechanism. Additionally, Storm requires bolt connections to be explicitly defined at start-up. This causes two disadvantages: Firstly, a single process cannot be updated or reconfigured without restarting the entire topology. Considerations should therefore be made on when to update the system and when to delay rolling-out an updated version. Secondly, the bolts are connected pair-wise. This is in contrast to most conventional publish/subscribe communication platforms (such as Kafka and RabbidMQ). These systems decouple the producer and consumers and instead write and read to addressable communication channels (topics). Storm allows reading and listening on streams of a certain topic, but the connection still needs to be explicitly specified. This is cumbersome, but should be able to be overcome. 
%Though cumbersome, this also grants an advantage. With strong component bindings it should prove more difficult to deploy an invalid architecture due to small mistakes such as mistypes or not updating all topic bindings on a refactor. 

\subsubsection*{Micro-component architecture without execution platform}
A final option is to employ a micro-component architecture without an execution platform. Instead, this requires distributing components manually and have them communicate using message brokers. This would increase the efforts required to develop and deploy the platform, but does provide greater control over its execution. Additionally this would alleviate the deficiencies identified for Apache Storm, such as difficult third party integration, cumbersome topology building and lack of run-time reconfiguration. 
%TODO runtime environment

\subsection{Message brokers}
%\subsubsection{Native HTTP}
%The Hypertext Transport Protocol (HTTP) \cite{def:http} is a [onmiskenbaar] communication standard this is widely employed in internet communications. It is well-documented and familiar to almost every industry professional, which should [ease] implementation and maintenance. Aside of maintainability HTTP is very versitile, which should ensure that it meets the needs for our system. However, this versitiliy stems from the barebone definition of the protocol. [TODO afmaken]. Additionally, HTTP routing is performed based on the IP address and port of the target process. This requires any sending component to know all its listener components, requiring either direct IP-based subscription or a discovery/lookup service. 
By employing a micro-component architecture (without inherent messaging system), a communication technology for components to communicate to each other needs to identified. This approach employs a service to which producers write messages to a certain topic. Consumers can subscribe to a topic and subsequently read from it. This obscures host discovery, since a producer need not know its consumers or vice versa. The routing is instead performed by the message service. The following will explore the two widely used message broker services in the industry.

\subsubsection*{RabbidMQ}
RabbitMQ \cite{web:rabbitmq} is a distributed open-source message broker implementation based on the Advance Message Queue Protocol. It performs topic routing by sending a message to an exchange server. This exchange reroutes the message to a server that contains the queue for that topic. A consumer subscribed to that topic can then retrieve it by popping it from the queue. Finally, an ACK is returned to the producer indicating that the message was consumed. The decoupling of exchange routers and message queues allows for custom routing protocols, making it a versatile solution. RabbitMQ operates on the \emph{competing consumers} principle, which entails that only the first consumer to pop the message from the queue will be able to consume it. This results in an \emph{exactly once} guarantee for message consumption. This makes it ideal for load-balanced micro-component applications, because it guarantees that a deployment of identical services will only process the message once. It does however make multi-casting a message to multiple types of consumers difficult.

\subsubsection*{Apache Kafka}
Converse, Apache Kafka \cite{web:kafka} distributes the queues itself. Each host in the cluster hosts any number of partitions of a topic. Producers then write to a particular partition of the topic, while consumers will receive the messages from all partitions of a topic. Because a topic is not required to reside on a single host, it allows load balancing of individual topics. This does however cause some QoS guarantees to be dropped. For instance, message order retention can no longer be guaranteed for the entire topic, but only for individual partitions. Kafka, in contrast to RabbidMQ's competing consumers, operates on the \emph{co-operating consumers} principle. It performs this by, instead of popping the head of the queue, a pointer is retained for each individual consumer. This allows multiple consumers to read the same message from a queue, even at different rates. The topic partition retains a message for some time or maximum number of messages in the topic, allowing consumers to read a message more then once. Ensuring that load-balanced processes only process a message once is also imposed on the consumer by introducing the notion of consumer groups. These groups share a common topic pointer, which ensures that the group collectively only consumes a message once. This process does not require an exchange service, so Kafka does not employ one. This removes some customization of the platform, but does reduce some latency. Lastly, Kafka does not feature application level acknowledgement, meaning that the producer cannot perceive whether its messages are consumed.

\subsubsection*{Comparison}
\input{resources/tables/rabbidmq-kafka}
A comparative summery of both technologies is given in table \ref{table:rabbidmq-kafka}. From this comparison the first apparent difference is the approach taken to consumer strategies. Kafka allows messages to be read multiple times, both by different consumers or the same consumer, whereas RabbidMQ allows messages to be consumed only once. Secondly, Kafka's lower-level replication provides increased scalability and speed \cite{kafka_vs_rabbitmq}. However, it does so at the cost of some functional benefits such as order retention, guaranteed delivery.

%Following this comparason we have chosen to employ Kafka for our platform. The first observation is that Kafka performs better in non-functional metrics. Sources report Kafka to be 2--4 times faster than RabbidMQ \cite{speed_kafka} and the partitioned topics allow Kafka to be distribute and scale overloaded channels. Secondly, the cooperating consumer model Kafka is based on allows us to natively multicast messages to multiple consumers, while still being scalable by defining consumer groups. By choosing for Kafka we do however default some features such as producer acknowledgement and topic level order guarantees. As for producer acknowledgement we do not require it, as producers simply send messages into the clear and consumers are required to make efforts that it processes all data. Using the feature to read messages more than once, we should be able to build a dependable platform. Finally, Kafka cannot guarantee the read order of partitioned topics. We therefore will need to enforce it ourselves in the platform and implementations of it. This can be either done by sorting messages in buffers on some ordered parameter (e.g. timestamp or sequence number) or by not partitioning topics containing order-critical streams.

\subsection{Distributed computing}
As specified by requirement \ref{r:basis_accumulated}, a means of processing large volumes of data is required. This is accomplished by aggregating a large number of snapshots into a distinct smaller amount of snapshots (often singular) with a higher-degree of information. In order to accomplish this a scalable means of computation is required (requirement \ref{r:scale}) 
\subsubsection{MapReduce}
MapReduce \cite{mapreduce} is a distributed computing framework. It operates by calling a \emph{mapper} function on each element in the dataset, outputting a set of key-value tuples for each entry. All tuples are then reordered, grouped as sets of tuples with a common key. The key-value sets are then distributed across machines and a \emph{reduce} function is called to reduce the many individual values into some accumulated data-points. The benefit of this framework is that the user need only implement the \emph{mapper} and \emph{reduce} functions. All other procedures, including tuple distribution and calling the mapper and reducer, are handled by the framework. An example of the algorithm on the WordCount problem is illustrated in Figure \ref{img:mapreduce}.

The concept of a mapped processor is of a large benefit to the platform. In the early exploration phase it quickly became apparent that there were many use cases where one might want to extract accumulated snapshots per individual sensor or grouped by cell tower. This approach also allows to compensate for devices sending at different rates. These devices would be overrepresented in the population if they were not normalized. By first grouping and averaging the messages per device, it can assured that every device has the same weight in the analysis.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{resources/img/mapreduce.png}
\caption{The overall MapReduce word count process \cite{mapreduce_img}}
%https://www.todaysoftmag.com/article/1358/hadoop-mapreduce-deep-diving-and-tuning
\label{img:mapreduce}
\end{figure}

Though the ease of implementation is very high and the technology is very applicable to the platform, the algorithm has proved to be comparatively slow. The reason for this is that before and after both the map and reduce phase the data has to be written to a distributed file system. Therefore though highly scalable, the approach suffers by slow disk writes \cite{mapreduce_vs_spark}. Finally, MapReduce works on large finite datasets. Therefore, the stream data must be processed into batches in order for MapReduce to be applicable.

\subsubsection{Apache Spark (Streaming)}
Apache spark \cite{web:spark} is an implementation of the Resilient Distributed Dataset (RDD) paradigm. It employs a master node which partitions large datasets and distributes it among its slave nodes, along with instructions to be performed on individual data entries. Operations resemble the functions and methods of the Java Stream package \cite{java_stream}. 

Three sort of operations exist: narrow transformations, wide transformations and actions. \emph{Narrow transformations} are parallel operations that effect individual entries in the dataset and result in a new RDD, with the original RDD and target RDD partitioned equally. Examples of such functions are \emph{map} and \emph{filter}. Because these transformations are applied in parallel and partitioning remains identical, many of these transformations can be performed sequentially without data redistribution or recalling the data to the master. \emph{Wide transformations} similarly are applied on individual dataset entries, but the target RDD may not be partitioned equal to the original RDD. An example of such a transformation is \emph{groupByKey}. Since elements with  he same key must reside in the same partition, the RDD might require reshuffling in order for computation to complete. Finally, Actions, such as \emph{collect} and \emph{count} require the data to be recalled to the master and final calculation is performed locally, resulting in a concrete return value of the process. RDD's provide an efficient distributed processing of large datasets, that is easy to write and read. However careful consideration must be given to the operations and execution chain in order to avoid superfluous dataset redistribution \cite{spark_programming_guide}.

\input{resources/listings/mapreduce_in_spark}

It is interesting to note that the MapReduce framework can easily be reproduced in Spark. this is achieved by calling the \emph{map} and \emph{reduceByKey} subsequently. To illustrate this the MapReduce procedure of Figure \ref{img:mapreduce} is implemented using Apache Spark in Listing \ref{list:mapreduce_spark-search}. Please note that the intermediate assignments of the RDD are not required. RDD operations can be chained after one another, but intermediate assignments have been used to better illustrate the steps taken. Also note that the first three steps are be performed fully parallized since they are all narrow transformations. Only line 5 (wide transformation) and 6 (action) require RDD redistribution.
% ref e.g.: https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-transformations.html

Additionally, the framework does not require disk writes as MapReduce does. Instead, it runs distributed calculations in-memory, thereby vastly improving the overall calculation speed. This does however raises a reliability issue, because if a slave node fails, its state cannot be recovered. Such occurrences are resolved by the master by replicating the part of the dataset from the intermediate result it retained and distributing it among the remaining slave nodes. Because the sequence of transformations is deterministically applied to each individual entry in the dataset any new slave node can continue calculations from the last point the state was persisted \cite{rdd_fault_tolerance}.

Finally however, Apache Spark suffers the same deficit as MapReduce and is performed on finite datasets. Therefore streams need to be divided in batches in order to perform calculations. Fortunately, such a library exists :Apache Spark Streaming \cite{web:spark_streaming}. It batches input from streams on regular intervals and supplies it to a Spark RDD environment. The time windows can be as small as a millisecond, therefore it is not formally real time, but can achieve near real-time stream processing \cite{dstreams}.


\subsection{Solution decisions}
\label{sec:solution_decision}
Apache Storm was chosen for a distributed component environment and messaging system. The reason for this was primarily that Storm was conceived with this type of real-time streaming micro-component application in mind. The spouts and bolts provide the perfect building blocks to design an iterative information refinement application with separation of concerns in mind, while the built-in streaming mechanism provides for the distribution needs. However, the lack of exposure for third party integration and the tedious process of specifying each and every component connection will have to be accounted for.

Though Storm contains the means for large scale snapshot aggregation, it will not be employed for it. Instead, the data aggregation will be supported by Apache Spark Streaming. The reason for this is that studies have shown Apache Spark to be upto 5 times faster than both MapReduce \cite{mapreduce_vs_spark} and Storm \cite{spark_vs_storm}. Spark does however have a larger latency, due to collecting batches of data instead of processing them real-time. This however should not cause a significant problem since the envisioned use case is for timed analysis jobs on very large amounts of input data, in order to detect collective tendencies of the system under investigation. For this scope of application the latency issues of Apache Spark do not impose a large deficiency.

Apache Kafka will be employed to facilitate external communication of the platform. The reason for this is its speed and greater scalability. Additionally, but to a a smaller degree, this was chosen because of Kafka's ability to multicast messages. This will allow multiple auxiliary processes to listen in on the proceedings of the platform. With the decision for Kafka comes another benefit, as the Spark Streaming library contains adapters for Kafka allowing direct connection to it. Therefore, data can simply be emitted to a Kafka topic and consumed by a Spark Streaming process. The greatest deficiency of Kafka, being the lack of topic-level order guarantee, is not of grave importance. The hindrance can be overcome by including timestamps or sequence numbers in the passed messages. Moreover, the Spark calculations most likely will not require order retention. The reason for this is that most computations will contain of a \emph{reduce} step, which requires the reduction operation to be both associative and commutative. Therefore the message order disregarded.

\section{Design of the software platform}
These technologies will be adopted by composing them using adapters and abstracting the solutions. The internal implementation details are shielded by abstracting the technologies, simplifying implementation by the user. Some scaffolds for bolts will be provided, intended for different types of data flows and data reductions. Additionally, these technologies are very abstract since they were intended for many unspecified usages. However the (to be developed) platform and group of target applications feature some known commonalities, which were previously considered variations. Therefore some functions can be implemented which were originally intentionally left unspecified. This will reduce the implementation effort required, again simplifying usage of the platform.

\subsection{Micro-component architecture}
The remainder of this section will explain what adaptations to the previously discussed technologies have been made.

\subsubsection*{Apache Storm}
The bulk of the processor (micro-component) construction, execution and messaging tasks of the platform will be performed by Apache Storm. However, as mentioned before, the process of specifying a processor topology in Storm is a cumbersome process due to the necessity of interconnecting each and every process individually. Therefore, cross-connecting $M$ producer components with $N$ consumers requires $M\cdot N$ explicitly specified connections. This is contrasted by technologies that employ topic based channels in which $M$ producers write to a channel to which $N$ consumers are subscribed, requiring but $M+N$ connections to be specified. To this end, a topology builder was developed which enables topic based streaming. The builder will automatically connect the specified components according to the topics they are subscribed to. In this manner a component and its connections can be specified with but a few instructions, as demonstrated in listing \ref{list:topologybuilder}. Note that the complexity of the topology does not impact the amount of code needed, as the code complexity is solely depended on the number of components and not how they are interconnected.

\input{resources/listings/storm_builder}

Since Storm allows processes to be duplicated for load-balancing purposes, it employs some methods of controlling which duplicated process worker will consume which snapshot. The two chief methods are supported by the platform. The first method is the \emph{shuffle grouping}. It is the simplest channel specification and does not offer any guarantees on which process worker will consume the snapshot. It is therefore described as receiver-agnostic. However this lack of guarantee will not effect most tasks since most will be stateless data processors. The second supported stream manipulation method is the \emph{field grouping}. It is used for processors that do retain a state or somehow require similar snapshots to always be processed by the exact same worker. An simple example of this is a processor that counts the number of snapshots received for each sensor in a WSN. If it cannot be guaranteed that all snapshots of a sensor \emph{S} are always processed by the same worker \emph{W}, one worker might count 40 snapshots and another would count 60 of them. This requires another singular processor that accumulates those counts in order to derive an accurate snapshot count. Therefore it is possible to specify a set of fields which will consistently determine which worker will consume a snapshot. In the developed platform this is specified at topic level. Again, to prevent repeated declarations. Therefore, each snapshot emitted to such a channel is required to include all fields specified in the field grouping of that channel.

Finally, though the abstractions and encapsulations of the Storm platform are believed to simplify implementation efforts, it could still be useful to an implementer to inject their own native Storm bolts or spouts. This might be due to reusing earlier defined bolts or requiring more control of a process than the abstraction offers. To this end, the developed topology builder encapsulates the topology builder provided by the Storm Java library. As a consequence, the topology builder, upon calling the \emph{build()} function, will return an instance of \emph{org.apache.storm.topology.TopologyBuilder}. This allows last-minute injection of self-specified native storm processes, before ultimately generating the Storm topology with that builder.

\subsubsection*{Incorporation of Apache Spark Streaming}
\label{sec:incorporation_spark}
As identified in by requirement \ref{r:basis_accumulated} there is a need to condense the information of enormous amounts of (individually) low-information snapshots into a diminished number of high-information snapshots. Additionally, the large amount of input snapshots and the assertion that the platform should be scalable (requirement \ref{r:scale}) entails that a scalable data accumulator should be made available. 

As specified in section \ref{sec:solution_decision} Apache Spark Streaming was chosen for this task. However, this causes an earlier identified problem: a direct incorporation of Apache Spark in Apache Storm is difficult. In order to solve this inoperability of interfaces it was decided to device a process that functions as an adapter between Storm and Spark. This adapter employs Apache Kafka, for which Spark does provide interfaces, to pipe snapshots obtained from Storm channels. Snapshots are then read from a Kafka channel and batches of snapshots are fed to Spark RDD computations. Once the cloud computations have concluded the data is returned to the Storm environment and aggregated snapshots are eventually forwarded to consecutive processes. This is achieved by deploying two Storm components. Firstly, a specialized Storm bolt named \emph{KafkaEmitter} is deployed. this process simply consumes Storm messages and forwards them to a Kafka channel. Secondly, a Storm spout is deployed which acts as a Spark driver program. This bolt contains the instructions for the distributed computation of the Spark cloud and results of the cloud computations will be returned to it. A graphical representation of this process is depicted in Figure \ref{fig:distributed_accumulator}.

\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{resources/img/distributed_accumulator.png}
\caption{Graphical depiction of the distributed accumulator process}
\label{fig:distributed_accumulator}
\end{figure}

Two interesting remarks should be made, as apparent from Figure \ref{fig:distributed_accumulator}. Firstly, The KafkaEmmitter can be replicated in order to prevent it being a point of congestion in the topology. Secondly, the fact that two distinct components (KafkaEmitter and Spark Master) are present is encapsulated by the topology builder. Developers need only declare an implementation of the distributed accumulator processor (acting as Spark master node) with the appropriate Storm and Kafka channels. The builder will then deploy a KafkaEmitter (or several) and the accumulator. This simplifies deploying the processor and obscures the internals by appearing as a single component.

\subsection{Scaffolds for micro-services}
With the technologies established the the component scaffolds that are supplied to application developers by the platform will be described. First, the base functions shared by all components will be described, before discussing them more in depth individually.

\subsubsection*{Common functionality}
Firstly, the components contain all functionality and information required to emit snapshots to subsequent components. A developer need only package the information in a snapshot consisting of key-value pairs and specify to which stream a snapshot must be emitted. The component then uses the information it received during the building of the topology to route the snapshot to all receivers subscribed to receive it. This not only implies routing the snapshot towards the correct component but also the correct component worker according to the defined field grouping.

Secondly, all components contain a base implementation of the \emph{prepare()} method. This method can be implemented to instantiate some properties that cannot be instantiated in the object's constructor. The reason that some properties cannot be instantiated in the constructor is that Storm processes (stouts and bolts) adhere to a pre-specified execution order. The component is:
\begin{enumerate}
\nospace
\item created by one of its constructors,
\item transmitted to one of the worker nodes of the Storm cluster,
\item further instantiated using the \emph{prepare()} method, and
\item executed according to its specification.
\end{enumerate}
The reason for this course of action is that step 1 is performed on the Storm master node, before distributing the functional object over the cluster. Therefore, during step 2 the object and its members need to be serializable. Non-serializable members are consequently instantiated during step 3, after the object has been transferred and before functional execution. The \emph{prepare(args)} method thus can be used to instantiate certain non-serializable properties. 

\subsubsection*{Spout}
This process mirrors the Apache Storm spout and is the component that introduces snapshots to the network. This component typically contains a handle to some external data source such as a database, API or auxiliary streaming technology. The reason for such a specific processor for this is the special execution cycle it has compared to a Storm bolt. Bolts execute with interrupts. They halt their execution until a new snapshot is available. However, a spout runs on an infinite-loop (until termination) continuously calling a method \emph{nextTuple()}. This method polls, retrieves and emits snapshot depending on the origin of the source.

\subsubsection*{SingleMessageProcessor}
This component is the most basic scaffold and closely resembles a Storm bolt. It however contains some additional functionality that improve its usability. It receives a snapshot and performs computations or analyses on it, before emitting new, enriched snapshots. Its typical use is for transformations of individual snapshots. As noted before this component requires implementation of a singular method: \emph{runForMessage(Message\ m)\emph} which will be called for each snapshot received by the component.

\subsubsection*{HistoricBufferedProcessor}
The HistoricBufferedProcessor resembles the SingleMessageProcessor in that it consumes single snapshots, but instead it processes or analyses an ordered series of relevant snapshots, called a \emph{window}. This is performed by retaining an in-memory buffer to which new snapshots are amended and is periodically filtered on relevance. This component can for example be used to determine recent trends in system parameters. The methods that require implementation for this component are \emph{runForBuffer(List\textless Message\textgreater\ l)}, which is run every time the buffer is updated, and \emph{cleanBuffer(List\textless Message\textgreater\ l)} which implements how and which elements should be pruned from the buffer, should they lose their relevance.

\subsubsection{DatabaseBufferedProcessor}
From a processing perspective the DatabaseBufferedProcessor is similar to the regular HistoricBufferedProcessor. It analyses a buffer of snapshots in order to emit a snapshot containing accumulated or averaged knowledge based on its input snapshots. However, rather then keeping an in-memory buffer of snapshots it maintains a connection with an database. This allows for buffered processing of snapshots that is not performed regularly, thereby not superfluously occupying memory resources.

To keep the component applicable to many database implementations and query languages it was chosen not to instil a database connection. Instead a developer is offered scaffolds to stepwise implement the intended behaviour with an actual  database connection. This scaffolding contains the methods processing the buffer (\emph{runForBuffer(List\textless Message\textgreater\ l)}) and purging the buffer (\emph{cleanBuffer(List\textless Message\textgreater\ l)}) as included in the HistoricBufferedProcessor. Aside from those functions it specifies function end-points for storing a new snapshot into the database and for fetching the relevant buffer from the database, respectively named \emph{persistMessage(Message m)} and \emph{fetchBuffer(Message m)}.	

\subsubsection{DistributedAccumulatorProcessor}
This component aggregates large amounts of laterally relevant snapshots. Laterally relevant entails that the snapshots describe similar data-points, but have little sequential relevance. The input for this process is a large amount of (individually) low-information snapshots. Conversely, the goal of the processor is to emit some high-information snapshot. An example of its usage is combining thousands of snapshots from sensors in order to obtain some collective application-level performance parameters. To accomplish the aggregation of these enormous amounts of data the accumulator principle described in section \ref{sec:incorporation_spark} is employed. By means of the method \emph{runForRange(JavaRDD\textless Message\textgreater\ rdd)} this component offers implementers a reference to the Spark RDD which contains all the snapshots collected during a pre-specified time period. The implementer can then use this RDD reference to sequentially manipulate and aggregate the collection of snapshots. Keeping proper parallelization in mind, this distributed component can perform data enrichment tasks on enormous batches of streaming data.

%A final remark to be made is on the granularity of the batch processing. Some real-time properties are lost by collection and processing streaming data as batches. This has been partly mitigated by employing the windowing mechanism of Apache Spark Streaming. This mechanism collects data in relatively small sub-RDDs. one or more of these smaller consecutive RDD's are then collected as one larger RDD called the 'window'. This window has a fixed size and slides over the sequence of sub-RDDs. This allows these small batches to be part of several consecutive windows. A graphical representation of this process is depicted in Figure \ref{fig:spark_window}. By this method it allows for example the analysis of data windows of the past 5 seconds, every one second. Whereas without this mechanism it would only be possible to process the last 5 seconds every 5 seconds or the last second every 1 second. Additionally, this process is very efficient, since the internal windowing mechanism automatically caches the results of the intermediary sub-RDD's. Therefore the entire chain of computations does not need to be recalculated for each windowed operation, only the transformations past the caching of the sub-results.

%\begin{figure}
%\centering
%\includegraphics[scale=0.55]{resources/img/spark-window.png}
%\caption{Apache Spark windowing mechanism. Source: \cite{spark_user_guide}}
%\label{fig:spark_window}
%\end{figure}

\subsubsection*{AccumulatorProcessor}
This component closely resembles the function of the above described DistributedAccumulatorProcessor, but is executed locally rather than on a cloud cluster. The purpose of this processor is tasks that would otherwise require the distributed accumulator, but whose limited scope be run in-memory on a single worker node. This could be a viable solution for applications that either run the accumulator task often enough or do not collect excessive amounts of snapshots. For these class of applications a locally executed accumulator task should prove sufficient and inclusion of such a components eliminates the base requirement of a Apache Spark cluster to be deployed in order for the platform to be executed, since the DistributedAccumulatorProcessor is the only component that employs it. It should however be noted that not deploying an accumulator in distributed mode could introduce a bottleneck in a Storm topology since the accumulator cannot be duplicated or load-balanced.

The processor was modelled after the MapReduce paradigm \cite{mapreduce} to guide its implementation. An implementer need only specify a \emph{map}, \emph{reduce} and \emph{collect} step.  The exact methods to implement for this are: 
\begin{description}[font=\normalfont]
\item[\emph{map(Message m) : String}] \hfill \\ Computes the key for a key-value snapshots.
\item[\emph{reduce(String key, List\textless Message\textgreater\ l) : Message}] \hfill \\ Reduces sets of key-value pairs grouped by key determined in the map step.
\item[\emph{collect(Map\textless String,Message\textgreater\ m) : Map\textless String,Message\textgreater}] \hfill \\ Collects the key-message pairs emitted by a reduce step. The return value of this method is a map of snapshots indexed by the Storm topic on which it should be forwarded.
\end{description}
Please note that the result of the reduce step is a set of snapshots. It is therefore possible to chain multiple map-reduce steps sequentially, as long as the sequence is concluded with a single collect step.

%TODO beter beschrijven zonder c4 te hoeven lezen
\subsubsection*{ResourceDistributionModelProcessor}
The final component is the ResourceDistributionModelProcessor. This processor is a special instantiation of the SingleMessageProcessor that analyses inbound snapshots according to a pre-specified ResourceDistributionModel (RDM). This model will be discussed in detail in Chapter \ref{ch:rdm}. In contrast to all other processors, this processor is not just a scaffold. Instead it executes completely automatically, requiring only an instantiation of an RDM and a specification of which model variables to output on which Storm channels. The processor then automatically provisions the input variables of the model, calculates the derived values and outputs the requested values as specified.

\section{Demonstration by example topology}
\label{sec:example_application_topology}

\begin{sidewaysfigure}
\centering
\includegraphics[width=\textwidth]{resources/img/example_topology.png}
\caption{Example topology of a platform implementation according to the example case}
\label{fig:example_topology}
\end{sidewaysfigure}

This section will demonstrate an example of a composition of the specified components. For this purpose case exemplified in section \ref{sec:example_case} will be considered. A graphical representation of the topology for the example implementation is depicted in figure \ref{fig:example_topology}. 

As the figure makes apparent, the application encompasses a large number of sensor devices. These devices regularly send their status information to the monitoring application via some external communication technology (e.g. Apache Kafka). These snapshots are introduced into the topology by \emph{SensorSpouts}. These spouts have been duplicated in order to accommodate the large amount of sensors which might send a sudden burst of data. The snapshots are then forwarded to the \emph{SensorProcessors} which have been provisioned with a Resource Distribution Model. This model consumes the measured parameters of the input snapshot and uses them to further calculate all the parameters which can be derived from the inputs, according to the specified model. This model also determines the optimal Resource Utilization Model (RUM) for this sensor device. Should no valid model composition be found this is reported to the \emph{NoRumActuator} which forwards a log message to the \emph{Reporter} component. The \emph{Reporter} will delegate the message to the correct reporting/alerting mechanism, outside of the topology. 

Should the current mode of operation be determined not to be optimal, the \emph{SensorProcessor} will report to the \emph{ChangeRumActuator}. The \emph{ChangeRumActuator} will report requests for change to an entity outside of the topology of the application. The actuator has been implemented as a DatabaseHistoricProcessor. The reason for this is that it will recollect the last few snapshots it received for this sensor and will only actually change the mode of operation of the sensor if it is consistent with the last few snapshots received. This eliminates superfluous communication with the sensor device caused by sporadic behaviour. Alternatively this component could have been implemented as a BufferedHistoricProcessor. However, a sensor is expected to send monitoring data only a few times per day and changes of operation would occur even less. It would therefore make little sense to keep a buffer of the last snapshots sent for each and every sensor in-memory. Additionally, this would have required a field grouping in case the component were to be load-balanced in order to enforce that the request for change of a particular sensor always be sent to the correct worker.

A final transformation to be performed is to infer application level intelligence from the low level sensor statuses. This is performed by the ApplicationAccumulator which collects data for a certain time period and calculates some high level data points, such as the measurement rate of the application averaged over its sensors, the total throughput and how many devices are performing on which RDM. This information is forwarded to the \emph{Reporter} which will make it available for visualization performed outside of the topology. Additionally the accumulator sends its aggregated snapshot to a \emph{TendencyAnalyser} which keeps a sequence of the total bandwidth used during previous time windows. Should this total consistently rise over a period of time, an alert will be sent by the reporter, as specified by the alerting requirements listed in section \ref{sec:example_case}.
	
\section{Discussion of the proposed software platform}
This section will evaluate the design of the monitoring platform. 

\subsubsection*{Satisfaction of requirements}
The first order of business is whether the proposed design satisfies the earlier stated requirements. The message-passing micro-service architecture provides the basis for snapshot transferral and transformation as stated in requirement \ref{r:snaptshot_transformation}. Furthermore, the requirements \ref{r:basis_single}, \ref{r:basis_historic} and \ref{r:basis_accumulated} are satisfied by the inclusion of the \emph{SingleMessageProcessor}, \emph{BufferedProcessors} and \emph{AccumulatorProcessors}, respectively. Finally, the last two requirements regarding the size of the applications in the problem domain and entailing scalability of the solution have been decisive for certain choices of the supporting technologies. For example, they it is reflected in the employment of cloud processing technology Apache Spark. From the aforementioned arguments it is concluded that every requirement is represented and met in the design of the platform.

\subsubsection*{Completeness with respect to QoI attributes}
The goal of the platform is to process and enrich data. It is therefore rational to evaluate the appropriateness and completeness of the platform by considering the information processing capabilities it offers. This section thusly evaluates the platforms completeness by demonstrating that the platform only positively impact the Quality of Information (QoI) of the input data. This entails that the QoI is improved or retained, but never lost as data passes through a platforms topology. This will be achieved by arguing the QoI parameters which were enumerated in section \ref{sec:back:qoi}. 

The first consideration of QoI is regarding the processing of data by the platform and affects the precision, completeness and usability of information. Firstly, \emph{precision} and \emph{certainty} are obtained by employing the HistoricProcessors. By averaging measurements anomalies are mitigated and the measured value closely approaches the norm of the measurements. Provided that the accuracy of the measurements is sufficient, this improved precision should consistently yield a measurement near the actual value. Secondly, the \emph{Usability} of information is improved as data moves throughout the topology. To illustrate this a thought experiment is proposed, using the example topology listed and described in section \ref{sec:example_application_topology} and a batch of raw data emitted during a certain time window. Before the data enters the platform it contains the potential to calculate the average throughput offered by the sensor application during that time window. Instead, this process is automated by an implementation of the platform and the resulting information is offered for further processing or visualization. This demonstrates that the platform can facilitate usability for information by calculating and producing ready-for-use values. It should however be noted that the \emph{completeness} of the information is greatly reduced during this process. To illustrate, from the average application throughput the throughput for individual devices can no longer be determined. For this reason, and others which will become apparent, committing the raw data to storage before processing it is recommended.

The second class of QoI attributes regards the processing efforts, expressed in time and costs. As the relevance of information degrades as time progresses, timely processing is paramount. \emph{Timely} execution is achieved by providing a scalable distributed solution. This ensures that, regardless of the intense information \emph{throughput}, the calculations can be performed in near real-time. Notice that only \underline{near} real-time is claimed, since Apache Spark collects records during a time window and performs calculations in batches. However the time window of such a batch can be set arbitrarily small for fine grained processing. Thereby it does not impact the timeliness significantly. However, adverse to this gained timeliness is a decreased \emph{affordability}. In order to incorporate these distributed cloud technologies a cluster of machines and increased development resources will need allocation. When the solution does not require this degree of scalability this poses an undue burden. Therefore locally deployable alternatives to these distributed processors are also provided. Implementations of the platform are therefore offered a trade-off between timeliness and cost.
	
Lastly, are the \emph{tunability} and \emph{reusability} of the information. Firstly, the data can be duplicated among different communication channels which allows differentiating calculations to be performed on the same data. Secondly, in order to facilitate evolution of end-user demands the platform has been designed with separation of concerns in mind. This allows continuous reconfiguration of the platform to be performed with reduced occurrence of concern entanglement. By redeploying the topology the same raw information can be used to facilitate updated user demands. This is also another reason to store the raw data before processing it. 
%By caching the data it can be re-fed into an updated topology in order to initialize an application as if it had been running for days.

Some final remarks should be made on the analysis. Firstly, the platform cannot offer any improvement or retention of information \emph{accuracy}, as it is solely determined by the method and quality of data measurement. Secondly, it should be noted that the platform does not assure preservation of any of these claim, since an implementation of the platform can violate any guarantee made. It can only be claimed that the platform does not impede any of the parameters and offers the means for developers to develop applications that do guarantee it.

\subsubsection*{Ease of adaptation}
A second point of focus is the ease of adoptation provided by the platform itself. It is asserted that low-level implementation details of Apache Storm and Spark are effectively obscured. This was achieved by offering some abstract components that require implementation of only a few methods. This obscuration entails a clearer programming interface to an implementer, as stated by the \emph{facade} software design pattern \cite{facade_pattern}. 

Secondly, the provided topology builder facilitates easy and fast building of a Storm topology. It does so by providing context aware topology and process instantiation, and topic based communication subscription and emission.  As mentioned before this allows $M$ producers and $N$ consumers connected by a single topic to be connected with complexity $\Theta(M+N)$, instead of the complexity $\Theta(M\cdot N)$ which would be required without the concept of topics.

These assertions will be formally validated in Chapter \ref{ch:validation}.

\subsubsection*{Technology stack}
Another issue to contemplate is the technology stack required for the platform. As mentioned in section \ref{sec:solution_decision}, Apache Storm was chosen as chief enabling technology. The main reason for this is that it offered most of the features required and would reduce the technology stack. However, by employing Apache Spark for distributed data aggregation, two additional cloud technologies are introduced. Spark itself and Kafka which is required in order to be connected to a Storm Topology. However, the inclusion of a distributed aggregation is necessary in order to keep the computations scalable. Additionally, the speed and efficiency arguments raised in section \ref{sec:solution_decision} justify the deployment of these additional technologies. Finally, when this scalability is not required Apache Spark and Kafka clusters can be executed locally on a single machine. This would still enjoy benefits from process parallelization, without requiring cluster deployment. Finally, Spark and Kafka may be omitted entirely, if permitted by the snapshot influx, as a non-distributed accumulator is also included in the platform.

%TODO \/ weg? ook al in validation en conclusion
\subsubsection*{Future work}
Finally, the topology-based separation of concern approach allows for visualization of the computations and distribution. The chain of computations can easily be depicted as a directed graph with processors and topics as nodes and processor-topic connections as vertices. Such a topology visualization would for example be very useful for identifying incorrectly or disconnected components. While runtime analysis tools are available for Storm topologies a graphical modelling/development tool is lacking. Such a tool would allow a topology to be drawn and functional methods to be implemented later. Though not featured, future efforts could be made to facilitate them.

%ease of use
%large technology stack
%qoi metrics
	%datastreams


%ease of use
%	topology build by 2 lines of code per component (4 if formatted)
%		create, register, declare, subscribe
%	total for test topology = xxx
	
%large stack -> also supply old accumulator
%also supply old
%		lower stack
%		gebruikt during development
%		map-reduce
%			rdd not map-reduce
%		Spark can run locally
%???	
%1. Every stream ends in distinct collection/number of streams -> therfore no n*m analysis necessary (only for each stream and for each analysis individually. (htis only descirbes outputs, not the inputs)
%2. there are no one/N to many relations. Implications?
%+RUM

%no mass emitter?
%??


%Discussion aan de hand van qoi metrics

%future work?
%composer GUI?



%architecture
%	model reification
%what components needed
%	featuremodel
%	requirements
%which candidates
%benchmarking
%desicions
