%TODO architecture vs. topolgy (at least define definition)
%TODO message vs snapshot
\newcommand{\archid}{1}
\chapter{Design of IoT monitoring platform architecture}
\label{ch:architecture}
%What needs to be monitored: sensor-networks/IoT 
%How data and information streams flow and combine 
%will focus on data sources and streams as to ascertain which architecture, configuration and components are needed/suitable.
In this chapter we will explain the process taken in order to device our general platform and its architecture. We will accomplish this by first exploring the general problem domain. We will then demonstrate why existing IoT monitoring platforms do not provide the services we require. We will then deliberate the design of our proposed platform and its implementation by identifying the available supporting technologies, clarifying the adaptations made to those technologies and explaining further implementation details. We will then conclude by discussing the success, applicability, disadvantages and deficits of our proposed solution.
\section{Goal}
Large sensor applications send immense amounts of low-level raw monitoring data that requires capturing and enrichment. Individual messages of raw data might contain very little information. However, these messages contain the potential from which meaningful conclusions can be derived, either on single sensor scale or about the sensor application as a whole. This raw data is enriched by combining and analysing datasets of similar, relevant data, in order to achieve a higher level of information. The goal of the efforts described in this chapter is to conceive a software platform that enables software developers to construct their own sensor application monitoring system. We intend to do this by devising a generic application backbone and base building blocks for  developers to extend and compose.  
\section{Conceptualization of the problem domain}
In this section we will investigate the problem domain in order to eventually determine the requirements for the model. We will achieve this by performing a commonality/variability analysis (C/V analysis) of the problem domain, as described in section \ref{sec:back:cv_analysis}. This analysis consists of three concepts:
\begin{itemize}
\item The definitions that will be used in the analysis and the remainder of this chapter, 
\item the common features of all elements in the problem domain which we may assume as established concepts, and 
\item the variations that occur between aspects of the problem domain for which we will need to account for in our proposed solution. Each point of variance needs to be accounted for in the requirements to be established.
\end{itemize}
\subsubsection*{Definitions}
We will start by defining some key terms that we will use in the analysis and the remainder of this chapter.
\begin{description}
\nospace
\item[Platform:] the monitoring platform to be designed.
\item[Application:] the application that is being investigated by the platform.
\item[Snapshot:] a message containing a collection of data-points indicating the state of a system on a certain instant.
\item[Source:] an entity emitting a snapshot. This can be a physical external device or an internal process.
\item[Consequence:] an action taken by the platform based on the analysis of one or more snapshots.
\end{description}
\subsubsection*{Commonalities}
With the definitions established we will continue to identify some common features shared by each application in the problem domain. These commonalities may be presumed during the design of our platform and grants focus to our efforts.
\begin{enumerate}[label=C\archid .\arabic*]
\nospace
\item \label{c:scale_sensor} The group of target applications involves a huge amount of sensors ([scale] which entails a high throughput of snapshots sent and requiring analysis by the platform.
\item \label{c:snapshot} As mentioned in the definitions data is captured in snapshots. These represent the state of (a part) of the application as measured or determined at a certain point in time. These snapshots can be used for both input of the platform as for representing intermediary states.
\item \label{c:snapshot_transformation} The parameters and values of a snapshot, and therefore consecutive derived values, may be considered fixed. Parameters can only change with the introduction of a new snapshot, not during evaluation of the current one.
\end{enumerate}
\subsubsection*{Variabilities}
Finally we will explore the variety within our problem domain. As the purpose of our solution is to process information we will mostly focus on the variables in the domain of information. Our solution should provide proficient adaptability in order to account for this variability. We ensure this by captivating these variations in requirements.
%TODO iets over QoI and information real-estate/capacity
\begin{enumerate}[label=V\archid .\arabic*]
\nospace
\item \label{v:qoi} the first variety we encountered is the variation in Quality of Information (QoI). As described in section \ref{sec:back:qoi} there are many parameters characterizing the QoI of data and QoI can vary on any combination of them.
\item \label{v:conclusion_basis} Secondly, there is the information base on which conclusions are made. The identified conclusion bases are:
\begin{enumerate}
\nospace
\item Single snapshot. (e.g. a sensor requiring maintenance)
\item Multiple sequentially relevant snapshots from a single source. Used to analyse tendency of parameters. (e.g. a sharp continuous increase in bandwidth used which may imply future capacity issues.)
\item Many multi-source snapshots without individual significance. E.g: while the individual throughput of sensors may be of little interest, knowledge of the average throughput of the system may be warranted.
\end{enumerate}
\item \label{v:consequence} The possible consequences by the platform have a large range of implementations and cannot be fully anticipated. Though the exact implementation of consequences can never be exactly anticipated, we can identify some groups of consequences.
\begin{enumerate}
\nospace
\item Build a model for reporting purposes. In order to generate reports some high-level information data-points need to be calculated based on (possibly multiple sequential) large datasets. these data-points are then exposed either by an in-memory component with an API or by persisting it to intermediary permanent storage.
\item Analysis which invokes an immediate feedback response to the application or a command \& control service administrating the application.
\item Alerting or reporting according to a specified rule. When this user defined rule is met or violated an alert is sent to an employee or auxiliary system.
\end{enumerate}
\end{enumerate}
%TODO confirm estiblishment in paper!
The final variety is the scale of the application. We have already established that the platform will operate on applications of large scale, i.e. thousands of sensors. However given a thousand as lower bound, the upper bound is still uncertain. therefore the size of the application is still uncertain and differing degrees of size require different computational needs.
\begin{enumerate}[label=V\archid .\arabic* , resume]
\nospace
\item \label{v:scale} The scale of large wireless sensor applications varies wildly. This yields for both the number of devices in the application and the rate at which the devices send data.
\end{enumerate}
\section{Requirements for the proposed software platform}
In this section we will describe the requirements for the proposed platform, in accordance with the variability identified in the previous section. 
\begin{enumerate}[label=R\archid .\arabic*]
\nospace
\item \label{r:snaptshot_transformation} The platform should enable the capture and transformation of snapshots.
\item \label{r:basis_single} The platform should enable processing of single snapshot.
\item \label{r:basis_historic} The platform should enable processing of a limited window of homogeneous snapshots.
\item \label{r:basis_accumulated} The platform should enable processing and aggregation of an enormous amount of snapshots.
\item \label{r:consequence} The platform should enable implementation of a wide range of consequences. It should at least provide for these anticipated types of consequence:
\begin{itemize}
\nospace
\item model building
\item application feedback
\item rule-based alerts
\end{itemize}
\item \label{r:scale} the platform should be scalable in order to support any large amount input devices
\end{enumerate}

\subsubsection*{Justification}
We will conclude this section by justifying the identified requirements according to the earlier performed C/V analysis. A formal traceability between the requirements, commonalities and variability is listed in table \ref{table:3_justification}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|} \hline
Requirement & Commonality/variability  \\ \hline
\ref{r:snaptshot_transformation} & \ref{c:snapshot}, \ref{c:snapshot_transformation}, \ref{v:qoi} \\ \hline
\ref{r:basis_single} & \ref{v:conclusion_basis}a \\ \hline
\ref{r:basis_historic} & \ref{v:conclusion_basis}b \\ \hline
\ref{r:basis_accumulated} & \ref{v:conclusion_basis}c \\ \hline
\ref{r:consequence} & \ref{v:consequence} \\ \hline
\ref{r:scale} & \ref{c:scale_sensor}, \ref{v:scale} \\ \hline
\end{tabular}
\caption{traceability table for justification of requirements}
\label{table:3_justification}
\end{table}

The first requirements regards the definition and concepts of snapshots and is based on the commonalities and the variation in QoI.  As illustrated by the traceability table the following three requirements closely correlate with the three varieties identified in \ref{v:conclusion_basis}. Requirement \ref{r:consequence} attempts to captivate the variability described in \ref{v:consequence}. This variation is captured in a single requirement as opposed to differenting them (as for \ref{v:conclusion_basis}), because the possible consequences are not limited to the identified consequence groups. Ultimately, the final requirement is regarding the scale of the target applications. This regards both the amount of devices in the target application as the frequency the send their snapshots. 

\section{Exploration of the solution domain}
In this section we will explore the solutions and supporting technologies that are offered to us. We will first consider the base architecture and backbone of the platform, as it is the most fundamental decision to be made. We will then continue to explore the options for message brokers, as a choice for a distributed architecture almost certainly requires one. We will conclude this chapter by examining some distributed cloud computing technologies that should allow us to perform expensive computations by distributing them over a cluster, as to provide the required scalability.
\subsection{Architecture basis and execution platform}
\subsubsection*{Monolithic architecture}
The first option to implement the platform is a monolithic software system. The benefit of such a system is that it keeps the solution as simple as can be. This is illustrated by a famous proverb of Dijkstra: "Simplicity is a prerequisite for reliability"\cite{zoeken}. This simplicity entails a better understanding of the product by any future contributor or user, without the need to consult complex, detailed documentation. However monolithic software products have been known to be difficult to maintain, because code evolution becomes more difficult as more and more changes and additions are made to the code base\cite{TODO:find}. Additionally, monolithic software systems are notoriously difficult to scale up and load balance\cite{TODO:find}, which violates requirement \ref{r:scale}. Therefore we will instead adapt a micro-component approach. Micro-component are  more flexible than monoliths, allow for better functional composition, are easier to maintain and much more scalable\cite{TODO:find}.

\subsubsection*{Apache Storm}
Apache Storm is a big data computing library especially designed for separation of concerns. It performs distributed computing by partitioning the stages of computation. By breaking up the computation, different stages can be distributed among machines and duplicated if need be. The Storm platform consists of three chief concepts.
\begin{description}
\nospace
\item[Spouts:] nodes that introduce data in the topology,
\item[Bolts:] nodes that perform some computation or transformation on data, and
\item[Streams:] connect nodes to one another and allows data to be transferred.
\end{description}
The computation is regarded as a directed graph with bolts as vertices, spouts as initial vertices and streams as edges.

Because data is emitted by spouts individually, Storm can achieve real-time processing of large amounts of data. By breaking up the computations into multiple consecutive bolts, Storm allows computations to be spread over a cluster. Additionally Storm allows individual bolts to be replicated and distributed. This lateral distribution prevents the occurrence of bottlenecks in the network due to bolts executing expensive pivotal processes

Storm is especially suited for our purpose since it was designed for microcomponents connected by streams. In contrast, many micro-component platforms focus on components exposing services which are explicitly invoked by other services\cite{refs: spring, etc}. By employing Apache Storm we obtain both the distributed computation environment as the means of data distribution, simplifying our technology stack.

Conversely however, the built-in stream distribution mechanism is completely internalized, making integration with auxiliary processes difficult. Tasks such as data injection, platform monitoring and data extraction for processing or reporting by third-party programs and stakeholders will require an exposing mechanism. Additionally, Storm requires bolt connections to be explicitly defined at start-up. This causes two disadvantages: Firstly, we cannot update or reconfigure a single process without restarting the entire system. Considerations should therefore be made on when to update the system and when to delay rolling out an updated version. Secondly, the bolts are connected in tuples. This is in contrast to conventional publish/subscribe communication platforms (such as Kafka and RabbidMQ) which decouple the producer and consumers and instead write and read to addressable communication channels called topics. Storm allows reading and listening on streams of a certain topic, but the connection still needs to be explicitly specified. This is cumbersome, but should be able to be overcome. Though cumbersome, this also grants an advantage. With strong component bindings it should prove more difficult to deploy an invalid architecture due to small mistakes as mistypes or not updating all topic bindings on a refactor. 
%TODO iets over real time?

\subsubsection*{Micro-component architecture without execution platform}
A final option is to employ a micro-component architecture without an execution platform. Instead we would deploy components ourselves and have them communicate using message brokers. This would increase the efforts needed to develop and deploy the platform, but does provide greater control over its execution. Additionally this would alleviate the deficiencies identified for Apache Storm, such as difficult third party integration, cumbersome topology building and lack of run-time reconfiguration. 

\subsection{Message brokers}
%\subsubsection{Native HTTP}
%TODO rewrite  to native data transfer
%The Hypertext Transport Protocol (HTTP)\cite{def:http} is a [onmiskenbaar] communication standard this is widely employed in internet communications. It is well-documented and familiar to almost every industry professional, which should [ease] implementation and maintenance. Aside of maintainability HTTP is very versitile, which should ensure that it meets the needs for our system. However, this versitiliy stems from the barebone definition of the protocol. [TODO afmaken]. Additionally, HTTP routing is performed based on the IP address and port of the target process. This requires any sending component to know all its listener components, requiring either direct IP-based subscription or a discovery/lookup service. 
%TODO fast: as fast as consumer/producer
By employing a micro-component architecture we need to identify a communication technology for components to communicate to each other. This approach employs a service to which producers write messages to a certain topic. Consumers can subscribe to a topic and consequently read from it. This obscures host discovery, since a producer need not know its consumers or vice versa. This routing is instead performed by the message service. The following will explore the two widely used message broker services in the industry: RabbidMQ.

\subsubsection*{RabbidMQ}
RabbidMQ\cite{web:rabbidmg} is a distributed open-source message broker implementation based on the Advance Message Queue Protocol. It performs topic routing by sending a message to an exchange server. This exchange reroutes the message to a server that contains the queue for that topic. A consumer subscribed to that topic can then retrieve it by popping it from the queue. Finally, an ACK is sent to the producer indicating that the message was consumed. The decoupling of exchange routers and message queues allows for custom routing protocols, making it a versatile solution. RabbitMQ operates on the \emph{competing consumers} principle, which entails that only the first consumer to pop the message from the queue will be able to consume it. This results in an \emph{exactly once} guarantee for message consumption. This makes it ideal for load-balanced micro-component applications, because it guarantees that a deployment of identical services will only process the message once. It does however make multi-casting a message to multiple types of consumers difficult.
%TODO refs

\subsubsection*{Apache Kafka}
Instead, Apache Kafka \cite{web:kafka} distributes the queues itself. Each host in the cluster hosts any number of partitions of a topic. Producers then write to a particular partition of the topic, while consumers will receive the messages from all partitions of a topic. Because a topic is not required to reside on a single host, it allows load balancing of individual topics. This does however cause some QoS guarantees to be dropped. For example message order retention can no longer be guaranteed for the entire topic, but only for individual partitions. Kafka, in contrast to RabbidMQ's competing consumers, operates on the \emph{cooperating consumers} principle. It performs this by, instead of popping the head of the queue, a consumer retains a counter pointing to its individual head of the queue. This allows multiple consumers to read the same message from a queue, even at different rates. The topic partition retains a message for some time or maximum number of messages in the topic, allowing consumers to read a message more then once. Ensuring that load-balanced processes only process a message once is also imposed on the consumer by introducing the notion of consumer groups. These groups share a common pointer, which ensures that the group collectively only consumes a message once. This process does not require an exchange service, so Kafka does not employ one. This removes some customization of the platform, but does reduce some latency. Lastly, Kafka does not feature application level acknowledgement, meaning that the producer cannot perceive whether its messages are consumed.
%TODO refs

\input{resources/tables/rabbidmq-kafka}
\subsubsection*{Comparison}
A comparative summery of both technologies is given in table \ref{table:rabbidmq-kafka}. Following this comparason we have chosen to employ Kafka for our platform. The first observation is that Kafka performs better in non-functional metrics. Sources report Kafka to be 2-4 times faster than RabbidMQ\cite{speed_kafka} and the partitioned topics allow Kafka to be distribute and scale overloaded channels. Secondly, the cooperating consumer model Kafka is based on allows us to natively multicast messages to multiple consumers, while still being scalable by defining consumer groups. By choosing for Kafka we do however default some features such as producer acknowledgement and topic level order guarantees. As for producer acknowledgement we do not require it, as producers simply send messages into the clear and consumers are required to make efforts that it processes all data. Using the feature to read messages more than once, we should be able to build a dependable platform. Finally, Kafka cannot guarantee the read order of partitioned topics. We therefore will need to enforce it ourselves in the platform and implementations of it. This can be either done by sorting messages in buffers on some ordered parameter (e.g. timestamp or sequence number) or by not partitioning topics containing order-critical streams.


\subsection{Distributed computing}
As specified by requirement \ref{r:basis_accumulated} we require a means of processing large amounts of data. We accomplish this by aggregating large numbers of snapshots into a distinct smaller amount of snapshots with higher-degree of information. In order to accomplish this we require a scalable means of computation (requirement \ref{r:scale}) 
\subsubsection{MapReduce}
MapReduce\cite{web:mapreduce} is a distributed computing framework. It operates by calling a \emph{mapper} function on each element in the dataset, outputting a set of key-value tuples for each entry. All tuples are then reordered, grouped by key as a key-value set tuple. The key-value sets are then distributed across machines and a \emph{reduce} function is called to reduce the many individual values into some accumulated data-points. The benefit of this framework is that the user need only implement the \emph{mapper} and \emph{reduce} functions. All other procedures, including calling the mapper and reducer, are handled by the framework. An example of the algorithm on the WordCount\cite{search} problem is illustrated in Figure \ref{img:mapreduce}.

The concept of a mapped processor is of a large benefit to our platform. In the early exploration phase it quickly became apparent that there were many use cases where one might want to extract accumulated snapshots per individual sensor or grouped by cell tower. This approach also allows to compensate for groups of devices sending more data then others. These devices would be overrepresented in the population if we did not account for them sending more messages than others. By first grouping the messages per device ID we can assure that every device has the same weight when we, fore example, calculate summations or averages.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{resources/img/mapreduce.png}
\caption{The overall MapReduce word count process\cite{mapreduce_img}}
%https://www.todaysoftmag.com/article/1358/hadoop-mapreduce-deep-diving-and-tuning
\label{img:mapreduce}
\end{figure}

Though the ease of implmenetation is very high and the technology is very appliccable to our platform, the algorithm has prooved to be comparatively slow. The reason for this is that before and after both the map and reduce phase the data has to be written to a distributed file system. Therefore though highly scalable, the approach suffers by slow disk writes\cite{mapreduce_vs_spark}. Finally, MapReduce works on large finite datasets. Therefore we need to manually preprocess stream data into batches in order for MapReduce to be applicable\cite{stream mapreduce}.

\subsubsection{Apache Spark (Streaming)}
Apache spark is an implementation of the Resiliant Distributed Dataset (RDD) paradigm. It entails a master node which partitions large datasets and distributes it among its slave nodes, along with instructions to be performed on individual data entries. Operations resemble the functions and methods of the Java Stream package \cite{java_stream_handleiding}. 

Three sort of operations exist: narrow transformations, wide transformations and actions. \emph{Narrow transformations} are parallel operations that effect individual entries in the dataset and result in a new RDD, with the original RDD and target RDD partitioned equally. Examples of such functions are \emph{map} and \emph{filter}. Because these transformations are applied in parallel and partitioning stays the same, many of these transformations can be performed sequentially without data redistribution or recalling the data to the master. \emph{Wide transformations} similarly are applied on individual dataset entries, but the target RDD may not be partitioned equal to the original RDD. An example of such a transformation is \emph{groupByKey}. Since elements with  he same key must reside in the same partition, the RDD might require reshuffling in order for computation to continue. Finally, Actions, such as \emph{collect} and \emph{count} require all data to be recalled to the master and most of the calculation is performed locally, resulting in a concrete return value of the process. RDD's provide an efficient distributed processing of large datasets, that is easy to write and read. However careful consideration must be given to the operations and execution chain in order to eliminate superfluous dataset redistribution.
%TODO refs

\input{resources/listings/mapreduce_in_spark}

It is interesting to note that the MapReduce framework can easily be reproduced in Spark. this is achieved by calling the \emph{map} and \emph{reduceByKey} consequtively. To illustrate we implemented the MapReduce procedure of Figure \ref{img:mapreduce} in Apache Spark using Java in Listing \ref{list:mapreduce_spark-search}. Please note that the individual assignments of the RDD are not required. RDD-calls can be chained after one another, but intermediate assignments have been used to better illustrate the steps taken. Also note that the first there steps are be performed fully parallized since they are all narrow transformations. Only line 5 (wide transformation) and 6 (action) require RDD redistribution.\cite{web:user_manual}
% ref e.g.: https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-transformations.html

Additionally, the framework does not require disk writes (as MapReduce does). Instead, it runs distributed calculations in-memory, thereby vastly improving the overall calculation speed. This does however raises a reliability issue, because if a slave node fails it cannot recover it's state. This is resolved by the master by replicating the part of the dataset from the intermediate result it retained and distributing it among the remaining slave nodes. Because the sequence of transformations is deterministically applied to each individual entry in the dataset any new slave node can continue calculations from that point.\cite{web:fault_tolerance}

Finally however, Apache Spark suffers the same deficit as MapReduce and is performed on finite datasets. Therefore streams need to be divided in batches in order to perform calculations. In fact a Apache Spark library exists (Apache Spark Streaming\cite{web:spark_streaming}) which performs in this manner. It batches input from streams on regular, pre-specified time intervals and supplies it to a Spark RDD environment. The time windows can be as small as a millisecond, therefore it is not formally real time, but can achieve near-real-time stream processing.

%TODO integration of communication techniques


\subsection{Solution decision}
\label{sec:solution_decision}
%TODO why niet jar-based with pub/sub
For distributed component platform we have chosen to build upon Apache Storm. The reason for this was primarily that Storm was conceived with this type of real-time streaming micro-component application in mind. The spouts and bolts provide us with the perfect building blocks to design an iterative information refinement application with separation of concerns in mind, while the built-in streaming mechanism provides the needs for a real-time distributed application. We will however need to account for the lack of expose points for third party integration and the tedious process of specifying each and every bolt connection.
%TODO meer meer meer. storm is awesome

Though Storm contains the means for large scale snapshot aggegation, we will not employ it. Instead we will base our data aggegation on Apache Spark Streaming. The reason for this is that studies have shown Apache Spark to be 5 times faster than both MapReduce\cite{spark-vs-mapreduce} and Storm\cite{spark-vs-storm}. Spark does however have a larger latency, due to collecting batches of data instead of processing them real-time. This however should not cause a significant problem since our envisioned use case is for timed analysis jobs on very large amounts of input data, in order to detect or visualise collective tendencies of the system under investigation. For this scope of application the latency issues of Apache Spark do not impose a large deficiency.

To facilitate external communication of the platform we will employ Apache Kafka. The reason for this is its speed and greater scalability. Additionally, but to a a smaller degree, this was chosen because of Kafka's ability to multicast messages. This will allow multiple auxiliary processes to listen in on the proceedings of the platform. With our choice for Kafka comes another benefit, as the Spark Streaming library contains adapters for Kafka allowing direct connection to it. Therefore we can simply emit data to a Kafka topic and connect a Spark Streaming process to it. The greatest deficiancy of Kafka, being the lack of topic-level order guarentee, is not of grave importance. The hindrence can be overcome by including timestamps or sequence numbers in the passed messages. Moreover, the Spark calculations most likely will not require order retention. The reaseon for this is that most computations will contain of a \emph{reduce} step, which requires the reduction operation to be both associative and commutative\cite{ass-comm}. Therefore the message order is of no importance.

\section{Design of the software platform}
%TODO java!!!
We will adapt these technologies by composing them using adapters and abstracting the solutions. By abstracting the technologies we shield the internal implementation details, simplifying implementation by the user. We will provide the implementer some scaffoldings for bolts intended for different types of data flows and data reductions. Additionally, these technologies are very abstract since they were intended for many unspecified usages. Since our platform and group of target applications features some known commonalities, which were considered variations when designing the original technologies, we can implement some functions which were originally intentionally left unspecified. This will reduce the implementation effort required, again simplifying usage of the platform. \cite{facade_pattern} 

%TODO onderscheid wanner het gaat over mijn platform en wanneer apache

\subsection{Micro-component architecture}
In the remainder of this section we will explain what adaptations to the previously discussed technologies are made.

\subsubsection*{Apache Storm}
%TODO define 'processor'
%TODO 'define topology ' and why it is cumbersome
%TODO facade pattern obscures/abstracts storm (kan ook hierboven)
The bulk of the component construction and execution, and streaming services of the platform will be performed by Apache Storm. However, as discussed before, the process of specifying a topology in Storm is a cumbersome process due to the necessity of interconnecting each and every process individually. Therefore, cross-connecting $M$ producer components with $N$ consumers requires $M\cdot N$ explicitly specified connections. This is contrasted by technologies that employ topic based channels in which $M$ producers write to a channel to which $N$ consumers are subscribed, requiring but $M+N$ connections to be specified. To this end we have developed a topology builder which enables topic based streaming. The builder will automatically connect the specified components according to the topics they are subscribed to, when executed. In this manner a component and its connections can be specified with but a few lines of code, as demonstrated in listing \ref{list:topologybuilder}. Note that the complexity of the topology does not impact the amount of code needed, as the code complexity is solely depended on the number of components and not how they are interconnected.

\input{resources/listings/storm_builder}

Since Storm allows processes to be duplicated for load-balancing purposes, it employs some methods of controlling which duplicated process worker will consume which messages. The two chief methods are supported by our platform. The first method is the \emph{shuffle grouping}. It is the simplest channel specification and does not offer any guarantees on which process worker will consume the message. It is therefore described as receiver-agnostic. However this lack of guarantee will not effect most tasks since most will be stateless data processors. The second supported stream manipulation method is the \emph{field grouping}. It is used for processors that do retain a state or somehow require similar messages to always be processed by the exact same worker. An simple example of this is a processor that counts the number of messages received for each sensor in a WSN. If we cannot guarantee that all messages of a sensor \emph{S} are always processed by the same worker \emph{W}, one worker might count 40 messages and another would count 60 of them. This would require another singular processor that accumulates those counts in order to derive an accurate message count. Therefore it is possible to specify a set of fields which will deterministically and consistently determine which worker will consume a message. In our adaptation this is specified at topic level, again to prevent repeated declarations. Therefore each snapshot emitted to such a channel is required to include all fields specified for that channel.

Finally, though we believe the abstractions and encapsulations of the Storm platform to be useful to simplify implementation efforts, it could still be useful to an implementer to inject their own native Storm bolts or spouts. This might be due to reusing earlier defined bolts or requiring more control of a process than our abstraction offers. To this end we have chosen our topology builder to encapsulate the topology builder provided by the Storm Java library. This entails that our topology builder, upon calling the \emph{build()} function, will return an instance of \emph{org.apache.storm.topology.TopologyBuilder}. This allows last-minute injection of self-specified native storm processes, before ultimately generating the Storm topology with that builder.

\subsubsection*{Incorporation of Apache Spark Streaming}
\label{sec:incorporation_spark}
As identified in by requirement \ref{r:basis_accumulated} there is a need to condense the information of enormous amounts of (individually) low-information snapshots into a distinct number of high-information snapshots. Additionally, the large amount of input snapshots, and the assertion that the platform should be scalable (requirement \ref{r:scale}) entails that we should make a scalable data accumulator available. 

As specified in section \ref{sec:solution_decision} we have chosen Apache Spark Streaming for this task. However this causes an earlier identified problem: a direct incorporation of Apache Spark in Apache Storm is difficult. In order to solve this inoperability of interfaces we have chosen to device a process that adopts the adapter software pattern \cite{search_ref}. This adapter employs Apache Kafka, for which Spark does provide interfaces, to pipe snapshots obtained from Storm channels. Snapshots are then read from a Kafka channel and batches of snapshots are fed to Spark RDD computations. Once the cloud computations have concluded the data is returned to the Storm environment and aggregated snapshots are eventually forwarded to consecutive processes. This is achieved by deploying two Storm components. Firstly, a specialized Storm bolt named \emph{KafkaEmitter} is deployed. this process simply consumes Storm messages and forwards them to a Kafka channel. Secondly, a Storm spout is deployed which acts as a Spark master node. This bolt contains the instructions for the distributed computation of the Spark cloud and results of the cloud computations will be returned to it. A graphical representation of this process is depicted in Figure \ref{fig:distributed_accumulator}.

\begin{figure}
\centering
\includegraphics[width=.7\textwidth]{resources/img/distributed_accumulator.png}
\caption{Graphical depiction of the distributed accumulator process}
\label{fig:distributed_accumulator}
\end{figure}

Two interesting remarks should be made, as apparent from Figure \ref{fig:distributed_accumulator}. Firstly, The KafkaEmmitter can be replicated in order to prevent it being a choke-point in the topology. Secondly, the fact that two distinct components (KafkaEmitter and Spark Master) are present is encapsulated by the topology builder. Developers need only declare an implementation of the distributed accumulator processor (acting as Spark master node) with the appropriate Storm and Kafka channels. The builder will then deploy a KafkaEmitter (or several) and the accumulator. This makes deploying the processor easier and obscures the internal implementation by appearing as a single component.

\subsection{Scaffolds for micro-services}
With the supporting technologies established we will now describe and deliberate the component scaffolds that are supplied for application developers by the platform. We will first describe the base functions shared by all components, before discussing them more in depth individually.

\subsubsection*{Common functionality}
Firstly, the components contain all functionality and information required to emit new snapshots to consequent components. A developer need only package the information in a message containing key-value pairs and specify to which stream a snapshot should be emitted. The component then uses the information it received during the building of the topology to route the snapshot to all receivers subscribed to receive it. This not only implies routing the snapshot towards the correct component but also the correct component worker according to the defined field grouping.

Secondly, all components contain a base implementation of the \emph{prepare(args)} \footnote{actual arguments have been omitted due to simplification} method. This method is used to instantiate some properties that cannot be instantiated in the objects constructor. The reason for this is that all components extend some abstract spout or bolt class of Apache Storm. In the Storm platform all spouts and bolts adhere to a pre-specified execution order. The component is:
\begin{enumerate}
\nospace
\item created by one of its constructors,
\item transmitted to one of the slave nodes of the Storm cluster,
\item further instantiated using the \emph{prepare(args)} method, and
\item executed according to its specification.
\end{enumerate}
The reason for this course of action is that step 1 is performed on the Storm master node, before distributing the functional object over the cluster. Therefore, during step 2 the object and its members need to be serializable. Non-serializable members are consequently instantiated during step 3, after the object has been transferred and before functional execution. The \emph{prepare(args)} method thus can be used to instantiate certain user-specified non-serializable properties. However, one should note that overwriting this method also requires invocation of the super method, since the default implementation specifies some non-serializable Storm properties and classes.

\subsubsection*{Spout}
This process is named after to the Apache Storm spout and is the component that introduces snapshots to the network. This component typically contains a handle to some external data source such as a database, API or streaming technology. The reason we need a special processor for this is the special execution cycle it has compared to a Storm bolt. Bolts execute with interrupts. They halt their execution until a new message is available. However, a spout runs on an infinite-loop (until termination) continuously calling a method \emph{nextTuple()}. This method polls, retrieves and emits messages depending on the origin of the source.

\subsubsection*{SingleMessageProcessor}
This component is the most basic scaffold and closely resembles a Storm bolt. It however contains some additional functionality that improve the ease-of-use. It receives a snapshot and performs computations or analyses on it, before emitting new, enriched snapshots. Its typical use is for transformations of individual snapshots. As noted before this component requires implementation of a singular method: \emph{runForMessage(Message\ m)\emph} which will be called for each key-value pair received by the component.

%TODO make indexable by key
\subsubsection*{HistoricBufferedProcessor}
The HistoricBufferedProcessor resembles the SingleMessageProcessor in that it consumes single snapshots, but instead it computes on or analyses a series of sequentially relevant snapshots, called the \emph{window}, sorted by sequence or time. This is performed by retaining an in-memory buffer to which new snapshots are amended and is periodically filtered on relevance. This component can for example be used to analyse and determine recent trends in system parameters. The methods that require implementation for this component are \emph{runForBuffer(List\textless Message\textgreater\ l)}, which is run every time the buffer is updated, and \emph{cleanBuffer(List\textless Message\textgreater\ l)} which implements how and which elements should be pruned from the buffer should they lose their relevance.
%TODO require field grouping?

\subsubsection{DatabaseBufferedProcessor}
TODO

\subsubsection{DistributedAccumulatorProcessor}
This component is used to aggregate large amounts of laterally relevant snapshots. By laterally relevant we mean that the snapshots describe similar data-points, but have no sequential relevance. The input for this process is a large amount of (individually) low-information snapshots in order to emit some high-information snapshots. An example of its usage is combining thousands of snapshots from individual sensors in order to obtain some collective performance parameters. For the task of accumulating and aggregating these enormous amounts of data we employ the accumulator principle described in section \ref{sec:incorporation_spark}. By means of the method \emph{runForRange(JavaRDD\textless Message\textgreater\ rdd)} this component offers implementers a reference to the Spark RDD which contains all the snapshots collected during a user-specified time period. The implementer can then use this RDD reference to sequentially manipulate and aggregate the collection of snapshots. Keeping proper parallelization in mind, this distributed component can perform data enrichment tasks on enormous batches of streaming data.
A final remark to be made is on the granularity of the batch processing. As stated before [(echt?)] some real-time properties are lost by collection and processing streaming data as batches. This has been partly mitigated by employing the windowing mechanism of Apache Spark Streaming. This mechanism collects data in relatively small sub-RDDs. one or more of these smaller consecutive RDD's are then collected as one larger RDD called the 'window'. This window has a fixed size and slides over the sequence of sub-RDDs. This allows these small batches to be part of several consecutive windows. A graphical representation of this process is depicted in Figure \ref{fig:spark_window}. By this method it allows for example the analysis of data windows of the past 5 seconds, every one second. Whereas without this mechanism it would only be possible to process the last 5 seconds every 5 seconds or the last second every 1 second. Additionally, this process is very efficient, since the internal windowing mechanism automatically caches the results of the intermediary sub-RDD's. Therefore the entire chain of computations does not need to be recalculated for each windowed operation, only the transformations past the caching of the sub-results.

\begin{figure}
\centering
\includegraphics[scale=0.55]{resources/img/spark-window.png}
\caption{Apache Spark windowing mechanism. Source: \cite{spark_user_guide}}
\label{fig:spark_window}
\end{figure}

\subsubsection*{AccumulatorProcessor}
This component closely resembles the function of the above described DistributedAccumulatorProcessor, but is executed locally rather than on a cloud cluster. The purpose of this processor is tasks that would otherwise require the distributed accumulator, but can instead be run in-memory on a single machine. This could be a viable solution for applications that either run the accumulator task often enough or do not collect excessive amounts of snapshots. For these class of applications a locally executed accumulator task should prove sufficient and inclusion of such a components eliminates the base requirement of a Apache Spark cluster to be deployed in order for the platform to be deployed, since the DistributedAccumulatorProcessor is the only component that employs it. It should however be noted that not deploying an accumulator in distributed mode could introduce a bottleneck in a Storm topology since the accumulator cannot be load-balanced. Load-balancing would require a sequential singular component that combines intermediary results aggregated by the load-balanced workers into an eventually final snapshot

To facilitate the easy implementation of the AccumulatorProcessor the processor was modelled after the MapReduce paradigm. An implementer need only specify a series of MapReduce steps (possibly singular) and an eventual single collect step.  The exact methods to implement for this are: 
\begin{description}[font=\normalfont]
\item[\emph{map(Message m) : String}] \hfill \\ Computes the key for a key-value message.
\item[\emph{reduce(String key, List\textless Message\textgreater\ l) : Message}] \hfill \\ Reduces sets of key-value pairs grouped by key determined in the map step.
\item[\emph{collect(Map\textless String,Message\textgreater\ m) : Map\textless String,Message\textgreater}] \hfill \\ Collects the key-message pairs emitted by a reduce step. The return value of this method is a map of messages indexed by the Storm topic on which it should be forwarded.
\end{description}
Please note that the return type for the reduce step is a new message. It is therefore possible to chain multiple map-reduce steps sequentially, as long as the sequence is concluded with a collect step.

\subsubsection*{ResourceDistributionModelProcessor}
%TODO here or in other chapter
[TODO]

\subsection{Demonstration by example topology}
\label{sec:example_application_topology}

\begin{sidewaysfigure}
\centering
\includegraphics[width=\textwidth]{resources/img/example_topology.png}
\caption{Example topology of a platform implementation according to the example case}
\label{fig:example_topology}
\end{sidewaysfigure}

In this section we will illustrate an example of the composition of the specified components. For this purpose we will consider the case exemplified in section \ref{sec:example_case}. A graphical depiction of the topology for the example implementation is found in figure \ref{fig:example_topology}. 

As figure \ref{fig:example_topology} makes apparent, the application encompasses a large number of sensor devices. These devices regularly send their status information to our application via some external communication technology (e.g. Apache Kafka). These snapshots are introduced into our topology by \emph{SensorSpouts}. These spouts have been duplicated in order to accommodate the large amount of sensors which might send a sudden burst of data. The snapshots are then forwarded to the \emph{SensorProcessors} which have been provisioned with a Resource Distribution Model. This model consumes the measured parameters of the input snapshot and uses them to further calculate all the parameters which can be derived from the inputs, according to the specified model. This model also determines the optimal mode of operation for this sensor device. Should no valid model composition be found this is reported to the \emph{NoRumActuator} which forwards a log message to the \emph{Reporter} component. The \emph{Reporter} will delegate the message to the correct reporting/alerting mechanism, outside of the topology. 

Should the current mode of operation be determined not to be optimal, the \emph{SensorProcessor} will report to the \emph{ChangeRumActuator}. The \emph{ChangeRumActuator} will report requests for change to an entity outside of the topology of the application. The actuator has been implemented as a DatabaseHistoricProcessor. The reason for this is that it will recollect the last few messages it received for this sensor and will only actually change the mode of operation of the sensor if it is consistent with the last few messages it received. In this manner we can eliminate superfluous expensive communication with the sensor device due to sporadic behaviour. Alternatively this component could have been implemented as a BufferedHistoricProcessor. However, a sensor is expected to send monitoring data only a few times per day and consequent a changes of operation would occur even less. It would therefore make little sense to keep a buffer of the last messages sent for each and every sensor in-memory. Additionally, this would have required a field grouping in case the component were to be load-balanced in order to enforce that the request for change of a particular sensor always be sent to the correct worker instance.

A final transformation to be performed is to infer application level intelligence from the low level sensor statuses. This is performed by the ApplicationAccumulator which collects data for a certain time period and calculates some high level data points, such as the measurement rate of the application averaged over its sensors, the total throughput and how many devices are performing on which RDM. This information is forwarded to the \emph{Reporter} which will make it available for visualization performed outside of the topology. Additionally the accumulator sends its aggregated snapshot to a \emph{TendencyAnalyser} which keeps a sequence of the total bandwidth during the time windows. Should this total consistently rise over a period of time or over a number of snapshots an alert will be sent by the reporter, as specified by the alerting requirements listed in section \ref{sec:example_case}.
	
\section{Discussion of the proposed software platform}
In this section we will evaluate the design of our monitoring platform. 

\subsubsection*{Satisfaction of requirements}
The first order of business is whether the proposed design satisfies the earlier stated requirements. we believe that the message-passing micro-service architecture provides the basis for snapshot transferral and transformation as stated in requirement \ref{r:snaptshot_transformation}. Furthermore, we believe that the requirements \ref{r:basis_single}, \ref{r:basis_historic} and \ref{r:basis_accumulated} are satisfied by the inclusion of the \emph{SingleMessageProcessor}, \emph{BufferedProcessors} and \emph{AccumulatorProcessors}, respectively. Finally, the last two requirements regarding the size of the applications in the problem domain and entailing scalability of the solution have been decisive for many choices of the supporting technologies and are reflected in our employment of cloud processing technology Apache Spark. From the aforementioned arguments we conclude that every requirement is represented and met in the design of the platform.

\subsubsection*{Completeness according to QoI attributes}
The goal of the platform is to process and enrich data. It is therefore rational to evaluate the appropriateness and compleness of the platform by considering the information processing capabilities it offers. In this section we will thusly evaluate the platforms completeness by demonstrating that the platform not only satisfied our identified requirements, but also does not negatively impact the Quality of Information (QoI) of the input data. By this we intend that the QoI is improved or retained, but never lost as data passes through the platforms topology. We will achieve this by arguing the QoI parameters which were enumerated in section \ref{sec:back:qoi}. 

The first consideration of QoI is regarding the processing of data by our platform and affects the precision, completeness and ease of use of information. Firstly, \emph{precision} and \emph{certainty} are obtained by employing the HistoricProcessors. By averaging measurements anomalies are mitigated and the measured value closely approaches the norm of the measurements. Provided that the accuracy of the measurements is sufficient, this improved precision should consistently yield a measurement near the actual value. Secondly, the \emph{ease of use} of information is improved as data moves throughout the topology. To illustrate this we propose a thought experiment using the example topology listed and described in section \ref{sec:example_application_topology} and a batch of raw data emitted in a certain time window. Before the data enters the platform it contains all the information potential to calculate the average throughput offered by the entire sensor application during that time window. Otherwise our platform equally would not be able calculate it. However, actually calculating it would involve extracting the correct data-point(s) from each snapshot, calculating device performance, extracting the throughput, averaging it for each device individually and ultimately calculating the average over the entire application. Instead this process is automated by an implementation of our platform and the resulting information is offered for further processing or visualization. This demonstrates that our platform can facilitate ease of use for information by calculating and producing a ready-for-use value. It should however be noted that the \emph{completeness} of the information is greatly reduced during this process. To illustrate, from the average application throughput the throughput for individual devices can no longer be determined. For this reason, and others which will become apparent, we recommend committing the raw data to storage before processing it.

The second class of QoI attributes regards the processing efforts, expressed in time and costs. As the relevance of information degrades as time progresses timely processing is paramount. We provide \emph{timely} execution by providing a scalable distributed solution. This ensures that, regardless of the intense information \emph{throughput}, the calculations can be performed in near real-time. Notice that we only claim \underline{near} real-time, since Apache Spark collects records during a time window and performs calculations in batches. However the time window of such a batch can be set arbitrarily small and the windowing mechanism of Spark allows for efficient fine grained processing, so it does not impact the timeliness greatly. However, adverse to this gained timeliness we have a decreased \emph{affordability}. In order to incorporate these distributed cloud technologies a cluster of machines and increased development resources will need allocation. When the solution does not require this degree of scalability this poses an undue burden. We have therefore also supplied the locally deployable alternatives to these distributed processors. Implementations of the platform are therefore offered a trade-off between timeliness and cost.
	
Lastly, we have the \emph{tunability} and \emph{reusability} of the information. Firstly, the data can be duplicated among different communication channel which allows differentiating calculations to be performed on the same data. Secondly, in order to facilitate evolution of end-user demands the platform has been designed with separation of concerns in mind. This allows continuous reconfiguration of the platform to be performed with reduced occurrence of concern entanglement. By redeploying the topology the same raw information can be used to facilitate updated user demands. This is also another reason to store the raw data before processing it. By caching the data it can be re-fed into an updated topology in order to initialize an application as if it had been running for days.

Some final remarks should be made on the analysis. Firstly, our platform cannot offer any improvement or retention of information \emph{accuracy}, as it is solely determined by the method and quality of data measurement. Secondly, it should be noted that our platform cannot assure preservation of any of these metrics, since an implementation of the platform can violate any guarantee made. It can only be claimed that the platform does not impede any of the parameters and offers the means for developers to develop applications that do guarantee it.

\subsubsection*{Ease of adaptation}
The first point of focus is the ease of adoptation provided by the platform itself. We believe that by offering some abstract components that require implementation of one or but a few methods, we have effictively obscured the low level implementation details of Apache Storm and Spark. This obscuration entails a clearer programming interface to an implementer, as defined by the \emph{facade} programming pattern. \cite{facade_pattern} 

Secondly, the provided topology builder facilitates easy and fast building of a Storm topology. It does so by providing context aware topology and process instantiation, and topic based communication subscription and emission.  As mentioned before this allows $M$ producers and $N$ consumers connected by a single topic to be connected with complexity $\Theta(M+N)$, instead of the complexity $\Theta(M\cdot N)$ which would be required without the concept of topics. This allows our example topology described in section \ref{sec:example_application_topology} can be specified using only [xxx] lines of code.

\subsubsection*{Technology stack}
The second issue to contemplate is the technology stack required for the platform. As mentioned in section \ref{sec:solution_decision} we chose Apache Storm as enabling technology because it offered most of the features required and would reduce our technology stack. However by employing Apache Spark for distributed data aggregation we have introduced two cloud technologies, as Spark requires Apache Kafka in order to be connected to a Storm Topology. We do however hold the belief that the inclusion of a distributed aggregator is necessary in order to keep the computation scalable. Additionally the speed and efficiency arguments raised in section \ref{sec:solution_decision} justify the deployment of these additional technologies. Finally, when this scalability is not required Apache Spark and Kafka clusters can be executed locally on a single machine, which would still enjoy benefits from process parallelization. Finally Spark and Kafka may be omitted entirely, as a non-distributed data aggregator is also included.

\subsubsection*{Future work}
Finally, our topology-based separation of concern approach allows for visualization of the computations and distribution. The chain of computations can easily be depicted as a directed graph with processors and topics as nodes and processor-topic connections as vertices. Such a topology visualization would for example be very useful for identifying incorrectly or disconnected components. With an even more extensive user interface an editor tool could be device, allowing a topology to be drawn and functional methods to be implemented later. It should be noted that, though promising, the library does not feature such visual user interfaces. However future efforts could be made to facilitate them.

%ease of use
%large technology stack
%qoi metrics
	%datastreams


%ease of use
%	topology build by 2 lines of code per component (4 if formatted)
%		create, register, declare, subscribe
%	total for test topology = xxx
	
%large stack -> also supply old accumulator
%also supply old
%		lower stack
%		gebruikt during development
%		map-reduce
%			rdd not map-reduce
%		Spark can run locally
%???	
%1. Every stream ends in distinct collection/number of streams -> therfore no n*m analysis necessary (only for each stream and for each analysis individually. (htis only descirbes outputs, not the inputs)
%2. there are no one/N to many relations. Implications?
%+RUM

%no mass emitter?
%??


%Discussion aan de hand van qoi metrics

%future work?
%composer GUI?



%architecture
%	model reification
%what components needed
%	featuremodel
%	requirements
%which candidates
%benchmarking
%desicions
